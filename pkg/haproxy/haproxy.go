package haproxy

import (
	"bytes"
	"context"
	"fmt"
	"html/template"
	"os"
	"os/exec"
	"path/filepath"
	"reflect"
	"strings"
	"sync"
	"time"

	"github.com/sirupsen/logrus"
)

// VIPConfig An HAProxy contains an IPV6 address, a set of pod IPs,
// the servicePort for the incoming traffic to the realserver, and the
// targetPort that endpoint pods use to rcv traffic
// that signify the service addresses & pod target port
type VIPConfig struct {
	Addr6 string

	PodIPs      []string
	TargetPort  string
	ServicePort string
	MTU         string
}

// IsValid determines if the VIPConfig is valid
func (v *VIPConfig) IsValid() bool {
	if len(v.PodIPs) == 0 {
		return false
	}

	if v.TargetPort == "" {
		return false
	}

	if v.ServicePort == "" {
		return false
	}

	return true
}

// HAProxySet provides a simple mechanism for managing a group of HAProxy services for
// multiple source and destination IP addresses. Specifically it provides a mechanism to
// create and reconfigure an HAProxy instance, as well as an instance to stop all running
// instances.
type HAProxySet interface {
	// Configure will create or update an HAProxy Instance.
	Configure(VIPConfig) error

	// StopAll will stop all HAProxy instances.
	// StopAll is blocking until all instances have been destroyed.
	StopAll()

	// StopOne will stop a single HAProxy instance.
	StopOne(listenAddrWithPort string)

	GetRemovals(v6Addrs []string) (removals []string)
}

// HAProxySetManager manages several HAProxy instances
type HAProxySetManager struct {
	sync.Mutex

	sources     map[string]HAProxy
	cancelFuncs map[string]context.CancelFunc
	errChan     chan HAProxyError

	binary    string
	configDir string

	cxl       context.CancelFunc
	ctx       context.Context
	parentCtx context.Context

	services map[string]string

	logger logrus.FieldLogger
}

// NewHAProxySet creates a new HAProxySetManager instance
func NewHAProxySet(ctx context.Context, binary, configDir string, logger logrus.FieldLogger) (*HAProxySetManager, error) {

	c2, cxl := context.WithCancel(ctx)
	defer cxl()

	// does the binary exist?
	// this still doesn't verify that the file is compiled correctly
	// and is an haproxy binary
	if !fileExists(binary) {
		return nil, fmt.Errorf("no file found at location %s specified for haproxy binary, exiting", binary)
	}

	// does the configDir exist? if not, make it
	if !dirExists(configDir) {
		cmdOutput, err := exec.CommandContext(ctx, "mkdir", "-p", configDir).Output()
		if err != nil {
			return nil, fmt.Errorf("unable to create config directory at %s: %v; %s", configDir, err, cmdOutput)
		}
	}

	return &HAProxySetManager{
		sources:     map[string]HAProxy{},
		cancelFuncs: map[string]context.CancelFunc{},
		errChan:     make(chan HAProxyError, 100),

		services: map[string]string{},

		binary:    binary,
		configDir: configDir,
		parentCtx: ctx,
		ctx:       c2,
		cxl:       cxl,

		logger: logger.WithFields(logrus.Fields{"parent": "haproxy"}),
	}, nil
}

// fileExists checks if a file exists and is not a directory before we
// try using it to prevent further errors.
func fileExists(filename string) bool {
	info, err := os.Stat(filename)
	if os.IsNotExist(err) {
		return false
	}
	return !info.IsDir()
}

func dirExists(dirName string) bool {
	_, err := os.Stat(dirName)
	return !os.IsNotExist(err)
}

// GetRemovals documented in HAProxySet interface
func (h *HAProxySetManager) GetRemovals(v6addrs []string) []string {

	// build a set of currently configured addresses
	h.Lock()
	configured := []string{}
	for addr := range h.sources {
		configured = append(configured, addr)
	}
	h.Unlock()

	// iterate over the inbound set.
	// any inbound address that is not in configured should be
	removals := []string{}
	for _, i := range configured {
		match := false
		for _, j := range v6addrs {
			if i == j {
				match = true
				break
			}
		}
		if !match {
			removals = append(removals, i)
		}
	}

	return removals
}

// StopAll cuaes the HAProxySetManager to stop all instances
func (h *HAProxySetManager) StopAll() {
	// TODO: block until all child instances are cleaned up
	h.logger.Debugf("StopAll called")
	h.cxl()

	// rebuild the internal state
	h.sources = map[string]HAProxy{}
	h.cancelFuncs = map[string]context.CancelFunc{}

	h.ctx, h.cxl = context.WithCancel(h.parentCtx)
}

// StopOne key generated by GetRemovals which iterates over h.sources
// call stop with prebuilt key
func (h *HAProxySetManager) StopOne(listenAddrWithPort string) {
	h.Lock()
	defer h.Unlock()
	h.logger.Debugf("StopOne called for %v", listenAddrWithPort)

	if cxl, ok := h.cancelFuncs[listenAddrWithPort]; ok {
		cxl()
		delete(h.cancelFuncs, listenAddrWithPort)
		delete(h.sources, listenAddrWithPort)
	}
}

// Configure creates an haproxy config from a given v6 backend and set of pods.
// It makes a config for each v6-addr:port combination the reason for this is
// that when a pod supporting a VIP goes away, the config MUST be rewritten so
// we don't route traffic to dead pod IPs. When this happens, HAProxy must reload
// the config which causes a (very brief) downtime for a server. A 1:1 map from
// backend to pod minimizes the detrimental effect of pod churn from a different
// service reloading a config for a service that isn't relevant to it
func (h *HAProxySetManager) Configure(config VIPConfig) error {
	listenAddr := config.Addr6
	podIPs := config.PodIPs
	targetPort := config.TargetPort
	servicePort := config.ServicePort
	mtu := config.MTU

	h.logger.Debugf("configuring s=%v d=%v tPort=%v sPort=%v", listenAddr, podIPs, targetPort, servicePort)
	h.Lock()
	defer h.Unlock()

	instanceKey := h.createInstanceKey(listenAddr, servicePort)

	// create the instance if it doesn't exist
	if _, found := h.sources[instanceKey]; !found {
		c2, cxl := context.WithCancel(h.ctx)
		instance, err := NewHAProxy(c2, h.binary, h.configDir, listenAddr, mtu, podIPs, targetPort, servicePort, h.errChan, h.logger)
		if err != nil {
			h.logger.Errorf("error creating new haproxy. canceling context. %v", err)
			cxl()
			return err
		}
		h.sources[instanceKey] = instance
		h.cancelFuncs[instanceKey] = cxl
	}

	// then configure it
	return h.sources[instanceKey].Reload(podIPs, targetPort, servicePort, mtu)
}

func (h *HAProxySetManager) createInstanceKey(listenAddr, servicePort string) string {
	return fmt.Sprintf("%s:%s", listenAddr, servicePort)
}

func (h *HAProxySetManager) run() {
	for {
		select {
		case <-h.ctx.Done():
			return
		case instanceError := <-h.errChan:
			h.logger.Errorf("got error from instance. %v", instanceError.Error)

			// delete the instance that's in an error state, then rebuild a new one and attach it to the sources set
			h.Lock()
			delete(h.sources, instanceError.Source)
			delete(h.cancelFuncs, instanceError.Source)
			c2, cxl := context.WithCancel(h.ctx)
			if instance, err := NewHAProxy(c2, h.binary, h.configDir, instanceError.Source, instanceError.MTU, instanceError.Dest, instanceError.TargetPort, instanceError.ServicePort, h.errChan, h.logger); err != nil {
				h.logger.Errorf("error recreating haproxy. canceling context. %v", err)
				cxl()
				h.errChan <- instanceError
			} else {
				h.sources[instanceError.Source] = instance
				h.cancelFuncs[instanceError.Source] = cxl
			}
			h.Unlock()

			// rate limit
			time.Sleep(1000 * time.Millisecond)
		}
	}
}

// HAProxyError represents an error from HAProxy
type HAProxyError struct {
	Error       error
	Source      string
	Dest        []string
	TargetPort  string
	MTU         string
	ServicePort string
}

// HAProxy defines what an HAProxy should be able to do
type HAProxy interface {
	Reload(podIPs []string, targetPort string, servicePort string, mtu string) error
}

// HAProxyManager manages a single running HAProxy instance
type HAProxyManager struct {
	binary     string
	configDir  string
	listenAddr string

	podIPs      []string
	targetPort  string
	servicePort string
	mtu         string

	rendered []byte
	template *template.Template

	cmd     *exec.Cmd
	errChan chan HAProxyError

	ctx    context.Context
	logger logrus.FieldLogger
}

type templateContext struct {
	TargetPort  string
	ServicePort string
	MTU         string
	Source      string
	DestIPs     []string
}

// NewHAProxy creates a new HAProxyManager instance
func NewHAProxy(ctx context.Context, binary string, configDir, listenAddr, mtu string, podIPs []string, targetPort, servicePort string, errChan chan HAProxyError, logger logrus.FieldLogger) (*HAProxyManager, error) {
	t, err := template.New("conf").Parse(haproxyConfig)
	if err != nil {
		return nil, err
	}

	h := &HAProxyManager{
		binary:     binary,
		configDir:  configDir,
		listenAddr: listenAddr,

		podIPs:      podIPs,
		targetPort:  targetPort,
		servicePort: servicePort,
		mtu:         mtu,
		errChan:     errChan,

		template: t,
		ctx:      ctx,
		logger:   logger,
	}

	// bootstrap the configuration. this is redundant with the operations in Reload()
	if b, err := h.render(podIPs, targetPort, servicePort, mtu); err != nil {
		return nil, fmt.Errorf("error rendering configuration. s=%s d=%v p=%v. %v", h.listenAddr, h.podIPs, targetPort, err)
	} else if err := h.write(b); err != nil {
		return nil, fmt.Errorf("error writing configuration. s=%s d=%v p=%v. %v", h.listenAddr, h.podIPs, targetPort, err)
	}

	// spin up the process
	go h.run()

	return h, nil
}

func (h *HAProxyManager) run() {
	//  fetch the pid of the parent process, if it exists
	pid, err := h.findPIDForProcess()
	if err != nil {
		h.logger.Warnf("failed to execute command to find pid: %v", err)
	}

	// args for normal instantiation of a haproxy instance
	args := []string{"-f", h.filename()}
	// did we find a running process for this filename?
	if pid != "" {
		// this set of arguments is proscribed by haproxy to run a zero downtime or "hitless" reload
		// detailed in https://www.haproxy.com/blog/truly-seamless-reloads-with-haproxy-no-more-hacks/
		// it is used to initialize the first run of an haproxy instance or a reload() interchangeably
		args = []string{"-f", h.filename(), "-p", "/var/run/haproxy.pid", "-sf", pid}
	}

	h.logger.Debugf("starting haproxy with binary %v and args %v", h.binary, args)
	cmd := exec.CommandContext(h.ctx, h.binary, args...)
	h.cmd = cmd

	cmdErr := make(chan error, 1)
	go func() {
		h.logger.Debugf("waiting for exit code")
		cmdErr <- cmd.Run()
		h.logger.Debugf("command exited")
	}()

	for {
		select {
		case <-h.ctx.Done():
			/*
				// Keeping this around as an example of how to gracefully shutdown when the parent context is closed.
				// In this case, HAProxy would progress through SIGUSR1, SIGTERM, finally SIGKILL. What's missing from this
				// is a way to communicate back to the caller that haproxy has been killed.
				// At any rate, get rid of CommandContext and instead deal with the complexity here. Implement HAProxy.Done()
				// or somesuch to deal with the communication factor.

				// if the context completes, the process needs to be stopped gracefully
				if err := h.cmd.Process.Signal(syscall.SIGUSR1); err != nil {
				        h.sendError(fmt.Errorf("haproxy could not receive sigusr1. s=%s d=%s p=%v. %v", h.listenAddr, h.serviceAddrs, h.ports, err))
				        return
				} else {
				        select {
				        case <-time.After(5000 * time.Millisecond):
				        case <-cmdErr:
				                return
				        }
				}

				// okay, so graceful shutdown didn't work. send SIGTERM
				if err := h.cmd.Process.Signal(syscall.SIGTERM); err != nil {
				        h.sendError(fmt.Errorf("haproxy could not receive sigterm. s=%s d=%s p=%v. %v", h.listenAddr, h.serviceAddrs, h.ports, err))
				        return
				} else {
				        select {
				        case <-time.After(2000 * time.Millisecond):
				        case <-cmdErr:
				                return
				        }
				}

				// kill the process
				if err := h.cmd.Process.Signal(syscall.SIGKILL); err != nil {
				        h.sendError(fmt.Errorf("haproxy could not receive sigkill. s=%s d=%s p=%v. %v", h.listenAddr, h.serviceAddrs, h.ports, err))
				        return
				}
				return
			*/

		case err := <-cmdErr:
			if err == nil {
				h.logger.Infof("exited without error due to reload or shutdown of process")
				return
			}
			e2 := fmt.Errorf("haproxy exited with error. s=%s d=%s p=%v. %v", h.listenAddr, h.podIPs, h.targetPort, err)
			h.logger.Errorf("wat. %v", e2)
			// the the command errors out, we need to report the error
			h.sendError(e2)
			return
		}
	}
}

// Reload rewrites the configuration and sends a signal to HAProxy to initiate the reload
func (h *HAProxyManager) Reload(podIPs []string, targetPort, servicePort, mtu string) error {
	// compare mtu, ports, and pods, and do nothing if they are the same.
	if mtu == h.mtu && targetPort == h.targetPort && servicePort == h.servicePort && reflect.DeepEqual(podIPs, h.podIPs) {
		return nil
	}

	// render template
	b, err := h.render(podIPs, targetPort, servicePort, mtu)
	if err != nil {
		return fmt.Errorf("error rendering configuration. s=%s d=%v p=%v. %v", h.listenAddr, h.podIPs, targetPort, err)
	}

	// write template
	if err := h.write(b); err != nil {
		return fmt.Errorf("error writing configuration. s=%s d=%v p=%v. %v", h.listenAddr, h.podIPs, targetPort, err)
	}

	// reload haproxy
	if err := h.reload(); err != nil {
		// if things go wrong, unroll the write
		h.unroll()
		return fmt.Errorf("unable to reload haproxy. s=%s d=%v p=%v. %v", h.listenAddr, h.podIPs, targetPort, err)
	}

	h.rendered = b
	h.targetPort = targetPort
	h.podIPs = podIPs
	h.servicePort = servicePort

	return nil
}

// render accepts a list of ports and renders a valid HAProxy configuration to forward traffic from
// h.listenAddr to h.serviceAddrs on each port.
func (h *HAProxyManager) render(podIPs []string, targetPort, servicePort, mtu string) ([]byte, error) {

	// prepare the context
	d := []templateContext{
		{
			TargetPort:  targetPort,
			ServicePort: servicePort,
			MTU:         mtu,
			Source:      h.listenAddr,
			DestIPs:     podIPs,
		},
	}

	// render the template
	buf := &bytes.Buffer{}
	if err := h.template.Execute(buf, d); err != nil {
		return nil, err
	}

	return buf.Bytes(), nil
}

// restart the process and overwrite the command context for this server
// run() performs a hitless reload of the haproxy, deciding on the fly
// whether a reload or first-run is needed
// this ends the parent process and starts a new one as well
func (h *HAProxyManager) reload() error {
	go h.run()
	return nil
}

// write replaces the existing configuration with the data stored in b, or else creates a new file.
func (h *HAProxyManager) write(b []byte) error {
	f, err := os.OpenFile(h.filename(), os.O_CREATE|os.O_RDWR|os.O_TRUNC, 0644)
	if err != nil {
		return err
	}
	defer f.Close()

	_, err = f.Write(b)
	return err
}

// filename returns the configuration filename, concatenating the configDir, the ipv6 address, and .conf
func (h *HAProxyManager) filename() string {
	return filepath.Join(h.configDir, h.listenAddr+"-"+h.servicePort+".conf")
}

// unroll is called by Reload when an error is generated after a new config file is written.
// It overwrites the file on disk with the former configuration.
func (h *HAProxyManager) unroll() {
	if err := h.write(h.rendered); err != nil {
		h.sendError(err)
	}
}

func (h *HAProxyManager) sendError(err error) {
	msg := HAProxyError{
		Error:       fmt.Errorf("unable to unroll haproxy config. config on disk and config in memory may be out of sync. s=%s d=%v. %v", h.listenAddr, h.podIPs, err),
		Source:      h.listenAddr,
		Dest:        h.podIPs,
		TargetPort:  h.targetPort,
		MTU:         h.mtu,
		ServicePort: h.servicePort,
	}
	select {
	case h.errChan <- msg:
	default:
		panic(err)
	}
}

// why do we do this bit?
// the recommendation for hitless reloads is to pass the output of /var/run/haproxy.pid to
// the -sf (-sf/-st [pid ]* finishes/terminates old pids) flag via $(cat <file>)
// however golang does not support command expansion from string arguments.
// alternatively I attempted code looking this: https://github.com/appscode/voyager/blob/0c747e307cf86c5ec0cd444dd509985bba5a3505/pkg/haproxy/controller/reload.go#L44
// but the fscan call failed as the PID was not populated, even when passing -p to the init process
// so now we have this icky way. oh well
func (h *HAProxyManager) findPIDForProcess() (string, error) {
	// empty string represents "pid not found", meaning no reload is required
	cmd := exec.CommandContext(h.ctx, "ps", "aux")
	b, err := cmd.CombinedOutput()
	if err != nil {
		return "", err
	}

	// this output is from the container, not the host os, so output will be
	// between 5-50 lines, I approximate
	pid := h.fetchPIDFromOutput(b)

	return pid, err
}

// broken out for unit testing
func (h *HAProxyManager) fetchPIDFromOutput(b []byte) (pid string) {
	psOut := strings.Split(string(b), "\n")
	for _, l := range psOut {
		// there can only be 1 instance of this in ps aux; will look like
		// haproxy -f h.filename() <args>, per the command generated in run()
		if strings.Contains(l, h.filename()) {
			spl := strings.Fields(l)
			if len(spl) >= 1 {
				pid = spl[0]
				break
			}
		}
	}

	return pid
}
